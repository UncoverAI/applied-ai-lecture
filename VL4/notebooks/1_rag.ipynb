{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'nomic-embed-text:latest',\n",
       "   'model': 'nomic-embed-text:latest',\n",
       "   'modified_at': '2024-11-16T13:50:03.4460443+01:00',\n",
       "   'size': 274302450,\n",
       "   'digest': '0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'nomic-bert',\n",
       "    'families': ['nomic-bert'],\n",
       "    'parameter_size': '137M',\n",
       "    'quantization_level': 'F16'}},\n",
       "  {'name': 'llama3.1:8b-instruct-q4_0',\n",
       "   'model': 'llama3.1:8b-instruct-q4_0',\n",
       "   'modified_at': '2024-11-16T13:48:48.5783556+01:00',\n",
       "   'size': 4661230766,\n",
       "   'digest': '42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "\n",
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_file_path: str):\n",
    "    text = \"\"\n",
    "    with open(pdf_file_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk the extracted text\n",
    "def chunk_text(text: str, chunk_size=500):\n",
    "    # Split text into chunks of specified size\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Compute dot product\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    \n",
    "    # Compute magnitude of the vectors\n",
    "    magnitude_vec1 = np.sqrt(np.dot(vec1, vec1))\n",
    "    magnitude_vec2 = np.sqrt(np.dot(vec2, vec2))\n",
    "    \n",
    "    # Avoid division by zero in case of zero vectors\n",
    "    if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity and find the top N matching chunks\n",
    "def find_top_n_chunks(question_embedding, chunk_embeddings, top_n=3):\n",
    "    # Calculate cosine similarities manually\n",
    "    similarities = [cosine_similarity(question_embedding, chunk_embedding) for chunk_embedding in chunk_embeddings]\n",
    "    \n",
    "    # Get the indices of the top N chunks with the highest cosine similarities\n",
    "    top_n_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    \n",
    "    # Return the indices and corresponding similarity scores\n",
    "    top_n_chunks = [(idx, similarities[idx]) for idx in top_n_indices]\n",
    "    return top_n_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to handle RAG\n",
    "def rag_ollama(pdf_file_path: str, questions: list, chunk_size: int, top_n: int,  model_name: str=\"nomic-embed-text:latest\"):\n",
    "    # Step 1: Extract text from the PDF\n",
    "    pdf_text = extract_text_from_pdf(pdf_file_path)\n",
    "    \n",
    "    # Step 2: Chunk the PDF text\n",
    "    pdf_chunks = chunk_text(pdf_text, chunk_size)\n",
    "    \n",
    "    # Step 3: Embed the PDF chunks\n",
    "    chunk_embeddings = ollama.embed(model=model_name, input=pdf_chunks)[\"embeddings\"]\n",
    "    \n",
    "    # Step 4: Embed the questions\n",
    "    question_embeddings = ollama.embed(model=model_name, input=questions)[\"embeddings\"]\n",
    "    \n",
    "    # Step 5: Find the top N matching chunks for each question\n",
    "    results = []\n",
    "    for i, question_embedding in enumerate(question_embeddings):\n",
    "        top_n_chunks = find_top_n_chunks(question_embedding, chunk_embeddings, top_n)\n",
    "        results.append({\n",
    "            'question': questions[i],\n",
    "            'top_chunks': [(pdf_chunks[idx], similarity) for idx, similarity in top_n_chunks]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_llm_conversation(question: str, top_chunks: list[str]):\n",
    "    messages = []\n",
    "    chunks = \"\\n\\n\".join([x[0] for x in top_chunks])\n",
    "    content = f\"\"\"You will always answer based on these chunks. If the user ask for a document, these chunks are the document:\\n{chunks}\\n\\n{question}\"\"\"\n",
    "    messages.append({\"role\": \"user\", \"content\": content})\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(question: str, top_chunks: list[str], model: str = \"llama3.1:8b-instruct-q4_0\"):\n",
    "    messages = convert_to_llm_conversation(question, top_chunks)\n",
    "    \n",
    "    return ollama.chat(model=model, messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main topic discussed in the document?\n",
      "Top 1 matching chunk (Score: 0.4559):\n",
      "suitable. Recent works such as Gandhi et al. (2024); Lehnert et al. (2024) have even suggested directly learning to optimally search and explore in reasoning tasks using meta-reinforcement learning. We believe this is a promising research direction for autonomous agents, which we will pursue in further work. Discrepancy between zero-shot vs search results. Similar to some recent works that focus on code and reasoning, we observe significant gap between zero-shot agent performance and performance of the agent equipped with search capabilities Brown et al. (2024); Snell et al. (2024). Investigating these trade-offs at scale and the potential effect of different search/optimization approaches. Online safety and interaction. The design of agent Q allows for largely autonomous exploration, self-evaluation and improvement with limited human intervention. However, the agent might make a significant number of mistakes in it’s search process which might be difficult to fix/reverse, especially for safety-critical online transactions, such as communications/email, payments, filings etc. This limits the scope of websites that Agent Q can be safely deployed and we might require additional safety critics and human-in-the-loop training setups. 15Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents References Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 1, 2023. Anthropic. Introducing the next generation of claude, 2024. URL IntroducingthenextgenerationofClaude . Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning, 2024. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence , 38(16):17682–17690, March 2024. ISSN 2159-5399. doi: 10.1609/aaai.v38i16.29720. URL http://dx.doi.org/10.1609/aaai.v38i16.29720 . Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787 . Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 2019. Yevgen Chebotar, Quan Vuong, Alex Irpan, Karol Hausman, Fei Xia, Yao Lu, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, Keerthana Gopalakrishnan, Julian Ibarz, Ofir Nachum, Sumedh Sontakke, Grecia Salazar, Huong T Tran, Jodilyn Peralta, Clayton Tan, Deeksha Manjunath, Jaspiar Singht, Brianna Zitkovich, Tomas Jackson, Kanishka Rao, Chelsea Finn, and Sergey Levine. Q- transformer: Scalable offline reinforcement learning via autoregressive q-functions, 2023. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Step-level value preference optimization for mathematical reasoning, 2024. URL https://arxiv.org/abs/2406.10858 . Wei Chen and Zhiyuan Li. Octopus v2: On-device language model for super agent, 2024. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. In NeurIPS Datasets and Benchmarks Track , 2023. URL https://openreview.net/forum?id=kiYqbO3wqw . 16Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2024. Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D. Goodman. Stream of search (sos): Learning to search in language, 2024. URL https://arxiv. org/abs/2404.03683 . Jonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown. Human-level performance in no-press diplomacy via equilibrium search, 2021. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. Reinforced self-training (rest) for language modeling, 2023. Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. In ICLR, 2024. URL https://openreview.net/forum?id=9JQtrumvg8 . Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model, 2023. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. ArXiv, 2024. URL https://api.semanticscholar.org/CorpusID:267211622 . Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, and Dorsa Sadigh. Contrastive preference learning: Learning from human feedback without reinforcement learning. In The Twelfth International Conference on Learning Representations , 2024. URL https: //openreview.net/forum?id=iX1RjVQODj . Samuel Holt, Max Ruiz Luyten, and Mihaela van der Schaar. L2mac: Large language model automatic computer for extensive code generation, 2024. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents. ArXiv, 2023. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents, 2022a. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter.\n",
      "\n",
      "Top 2 matching chunk (Score: 0.3730):\n",
      "Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models, 2023. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476–15488, 2022. Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, YannLeCun,YiMa,andSergeyLevine. Fine-tuninglargevision-languagemodelsasdecision-making agents via reinforcement learning, 2024. Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, and Saravan Rajmohan. Ufo: A ui-focused agent for windows os interaction. arXiv, 2024a. URL https://api.semanticscholar.org/CorpusID:267211622 . Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users, 2023. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Ap- pagent: Multimodalagentsassmartphoneusers. arXiv,2024b. URL https://api.semanticscholar. org/CorpusID:262053313 . Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, and Caiming Xiong. Agentohana: Design unified data and training pipeline for effective agent learning, 2024c. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges, 2024d. XuanZhang,ChaoDu,TianyuPang,QianLiu,WeiGao,andMinLin. Chainofpreferenceoptimization: Improving chain-of-thought reasoning in llms, 2024e. XuanZhang,ChaoDu,TianyuPang,QianLiu,WeiGao,andMinLin. Chainofpreferenceoptimization: Improving chain-of-thought reasoning in llms, 2024f. URL https://arxiv.org/abs/2406.09136 . Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. ArXiv, 2023. URL https://api.semanticscholar.org/CorpusID:262053313 . Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models, 2024a. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environ- ment for building autonomous agents. In ICLR, 2024b. Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl, 2024c. 22\n",
      "\n",
      "Top 3 matching chunk (Score: 0.3609):\n",
      "to sparsity of environment rewards. Indeed even a small mistake across the trajectory can cause the final agent output to be wrong, creating significant credit assignment problems. To overcome this, we use AI feedback (Bai et al., 2022) and self-criticism (Yuan et al., 2024) to further prompt the LLM to provide self-evaluation feedback at each node, which serves as intermediate reward and helps guide the search steps. This meaningfully improves the final agent success rate, but requires significant online interactions and moreover the capability to rollback actions, which is not always possible in online realistic settings. Such online autonomous search with little supervision on the web can result in a weak or unsafe agent which can make many errors, resulting in risky behaviors in sensitive online settings like bank transfers and sensitive information sharing. To correct this, we use the traces generated by the search process to improve capabilities of the model by learning from both the successful and unsuccessful trajectories with offline reinforcement learning, utilizing the Direct Preference Optimization (DPO) algorithm. We create preferences over different branches at the node level, which are scored using a mixture of the AI process feedback rewards and the final success rate of the explored branch. We evaluate our approach on the simulated WebShop benchmark (Yao et al., 2022)—a simulated e-commerce platform—as well as a real-world reservations booking website. We utilize LLaMa 3-70B as the base model in our experiments. In the WebShop environment, our approach consistently outperforms behavior cloning and reinforcement learning fine-tuned baselines, and beats average human performance when equipped with the capability to do online search. Inourreal-worldbookingexperiments, usingour Agent Q frameworkweimprovethemodelzero-shot absolute success rate from 18.6%to81.7%(a340% relative increase ), outperforming GPT-4’s performance after a single day of autonomous data collection. When we equip Agent Q with online search capability, our absolute success further improves to 95.4%. We believe that our approach represents a significant step forward in the development of autonomous web agents through it’s search and self-critique capabilities, setting a new benchmark for reliable multi-step decision-making in interactive settings. 2. Related Work Our work touches on a large number of research directions around agent design, self-improvement, reasoning and reinforcement learning. We include a short overview of related works from those various fields below. 2.1. Guided Search for Reasoning and Planning The latest generation of Large Language Models (LLMs) have demonstrated promising emerging properties around reasoning and planning. Moreover such behaviours can be directly elicited from strong models only using simple prompting techniques (Kojima et al., 2022; Qiao et al., 2023; Wei et al., 2022). These have also become an integral part of agentic design (Yao et al., 2023b; Zhang et al., 2024c), which we also utilize for our approach. Another emerging research direction is based around step-by-step verifiers or “Process Reward Models” (Lightman et al., 2023; Uesato et al., 2022), specifically for mathematical reasoning. These have shown to improve performance beyond purely outcome-based training, however they require a large amount of human effort to label individual steps. Some recent approaches have proposed self-supervised methods for step-level supervision (Hwang et al., 2024; Setlur et al., 2024a; Wang et al., 2024b). A number of concurrent works (Tian et al., 2024; Xie et al., 2024; Zhang et al., 2024e) have further explored tree-based search approaches 3Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents (Yao et al., 2023a) in combination with DPO (Rafailov et al., 2023) training for math-based reasoning. These algorithms optimize actions at the node level, using different branches produced by the search algorithm to create preference pairs. Our approach shares similarities to the self-supervised search proposed in (Yao et al., 2023a) with a combination of AI-based feedback (Bai et al., 2022; Yuan et al., 2024) to guide intermediate search steps, but we are the first to scale this a realistic agent setting. Similar approaches were proposed in (Hao et al., 2023; Zhou et al., 2024a), and other works (Koh et al., 2024); however these works only use the base model’s zero-shot capability and do not train it further. Moreover they are only evaluated on simulated environments. Beyond the search stage, our work further adopts the training methodology of (Tian et al., 2024; Xie et al., 2024; Zhang et al., 2024e), which significantly boosts our agent’s zero-shot capabilities. 2.2. Web Agents The strength and capabilities of recent pretrained Large Language (Vision) Models LL(V)Ms has significantly boosted progress in developing autonomous web-agents. Improved code understanding and long context have allowed agents to represent environment state and action space with document object model (DOM) allowing for deployment in complex and realistic domains. Moreover strong reasoning (Yao et al., 2023b) and planning (Liu et al., 2023; Zhang et al., 2024c) capabilities have also led to the development of a number of promising agents (Deng et al., 2023; Gur et al., 2024; Hong et al., 2023; Zhang and Zhang, 2023; Zhou et al., 2024b). Beyond using LL(V)Ms as plug-and-play planners/policies, recent works have sought to improve agentic-specific performance. Examples include online exploration (Zhang et al., 2024a), planning (Zhang et al., 2024b), error-correction (Wang et al., 2024a), and self- (Wu et al., 2024) or AI-critique (He et al., 2024; Pan et al., 2024). However, with small exceptions (Nakano et al., 2022) (which is still limited in scope) these agents mostly provide a framework around a strong pre-existing model like GPT4-V or deploy limited fine-tuning and adaptation. In this work we show that model training is crucial for continuous improvement. We combine a planning and reasoning agent with MCTS inference-time search and AI self-critique for self-supervised data collection, which we then use for RL type training. 2.3. Reinforcement Learning for LLMs and Agents ReinforcementLearninghasbecomeasignificantcomponentoftrainingmoderngenerativeAIsystems (Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023). Classical approaches have deployed the PPO algorithm (Schulman et al., 2017)—or similar policy-gradient based methods—and have even been scaled to autonomous web search agents (Nakano et al., 2022) as well as embodied applications with vision-language models (Zhai et al., 2024) (in simulation). However, these algorithms are challenging due to\n",
      "\n",
      "The main topic of the document appears to be the development of autonomous web agents through a combination of planning and reasoning capabilities, as well as reinforcement learning. Specifically, the document discusses a novel approach for training language models (LLMs) to perform complex decision-making tasks on the web by using Model-Tree Search (MTS) with AI feedback and self-criticism.\n",
      "--------------------------------------------------\n",
      "Question: Can you explain the methodology used?\n",
      "Top 1 matching chunk (Score: 0.5205):\n",
      "tree from the root node to a leaf node using tree policy (UCB1; 7) Trajectory Rollout : From the selected node’s trace, roll out the trajectory using 𝜋𝜃𝑖until a terminal state is reached Backpropagation: Backpropagate the value estimate bottom-up (8) end for Collect trajectories from rollouts and store them in replay buffer ℬ end for Construct preference pairs 𝒟𝑃={(h𝑡,a𝑤 𝑡,a𝑙 𝑡)}𝑇−1 𝑡=1where h𝑡∼𝒟 𝑃. For each node at step level 𝑡, compare each pair of child nodes, and construct the pair of generated actions (a𝑤,a𝑙)if the values of taking the action, |𝑄(h𝑡,a𝑤)−𝑄(h𝑡,a𝑙)|> 𝜃threshold, where 𝑄(h𝑡,a𝑤)and𝑄(h𝑡,a𝑙)are computed using (10) Optimize LLM policy 𝜋𝜃𝑖using DPO objective in Eq. (5) with 𝒟𝑃and𝜋ref end for (and 𝑁(h𝑡) =∑︀𝐾 𝑖=1𝑁(h𝑡,a𝑖 𝑡)). The backpropogation updates correctly maintain these values. 5.2. Improving Zero-Shot Performance with Reinforcement Learning Training large foundation models with offline Snell et al. (2022) or off-policy Chebotar et al. (2023) reinforcement learning at scale has still remained challenging. At the same time online (on-policy) reinforcement learning Ouyang et al. (2022); Stiennon et al. (2022) is not scalable to real interactive environments. Instead, we follow a line of recent works, which apply the DPO algorithm Rafailov et al. (2023, 2024) at the step level in multi-step reasoning problems in mathematical domains Chen et al. (2024); Hwang et al. (2024); Lai et al. (2024b); Lu et al. (2024); Setlur et al. (2024b); Xie et al. (2024); Zhang et al. (2024f). Our approach is most similar to Chen et al. (2024); Xie et al. (2024); Zhang et al. (2024f) who also use the branching nature of tree search to produce step-level preference pairs. We will also use this approach in our setting due to its simplicity, scalability and prior success in smaller scale (non-interactive) reasoning applications. We will generate a dataset of preference pairs 𝒫={h𝑡,a𝑤 𝑡,a𝑙 𝑡}where we make sure both actions were explored. We then optimize the DPO objective in Eq. 5 on the node level. We will leverage a theoretical result below to guide the construction of these preferences. We can make a number of modifications to Theorem 6.1 from Setlur et al. (2024b) to incorporate the interactive nature of the web environment dynamics to obtain the following result: Theorem 1. Consider a policy that optimizes the objective in Eq. 3 on trajectories generated by 𝜋ref and that at each node h𝑡we have preferences generated accordingly to 𝑝(a𝑤 𝑡≻a𝑙 𝑡|h𝑡)∝𝜎(𝑄(h𝑡,a𝑤 𝑡)− 11Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents 𝑄(h𝑡,a𝑙 𝑡)), then the policy which optimizes the DPO objective in Eq. 5 is identical to the optimal RL policy 𝜋*(a|h𝑡)∝𝜋ref(a|h𝑡) exp ( 𝑄(h𝑡,a)/𝛽) (9) Proof.The proof follows directly from the proof of Theorem 6.1 in Setlur et al. (2024b) and the control as inference arguments in Levine (2018); Rafailov et al. (2024). That is, we can approximate the optimal RL policy if we generate preferences under the optimal value function (or an approximation thereof). Since the outcome success provides limited supervision we also incorporate process supervision through the AI feedback as outlined in Section 5.1.1. We interpret the ranking of possible actions by the model to be driven by an implicit value function. Similar semantics was used in Koh et al. (2024), where GPT-4 was used as a zero-shot value function, while here we ask the model to instead reason over the given potential actions and provide rankings instead. This self-rewarding approach has shown promise in the RLHF setting Yuan et al. (2024) and we utilize it for our agent setting as well. Under this formulation, we compute the state-action value as an average: 𝑄(h𝑡,a𝑖 𝑡) =𝛼˜𝑄(h𝑡,a𝑖 𝑡) + (1−𝛼)ˆ𝑄(h𝑡,a𝑖 𝑡) (10) where ˜𝑄(h𝑡,a𝑖 𝑡)is the empirical value estimated through MCTS backpropagation and ˆ𝑄(h𝑡,a𝑖 𝑡)is a valueestimatebasedontherankingoftheaction a𝑖 𝑡bytheprocesssupervisionAImodel. Wethencreate preferences over pairs of actions which are above a certain value threshold |𝑄(h𝑡,a𝑤 𝑡)−𝑄(h𝑡,a𝑙 𝑡)|≥ 𝜃threshold. The full outline of our RL approach is shown in Algorithm 1. 5.3. Full WebShop Results The full range of results and baselines is shown in Figure 3. We see that equipping the agent with searchcapabilitiesattesttimesignificantlyboostsuccessratesfrom28.6%to48.4%whenusingMCTS on top of the base xLAM-v0.1-r model, approaching close to the average human performance of 50.0% and significantly out-performing the zero-shot performance of the DPO model trained with outcome supervision. We further fine-tune the base model using the approach outlined in Algorithm 1, which yields an improvement of 0.9% over the base DPO model. Using MCTS on top of the trained Agent Q model further improves performance to 50.5% slightly out-performing the average human success rates. We find that the ability to search at test time is a significant paradigm shift from zero-shot agents, even with significant RL training. Furthermore, while dense-level supervision improves over purely outcome-based one, the improvement is modest on WebShop. This is because the environment requires relatively short trajectories, and the model is capable to learn credit assignment purely from outcome supervision. We will further explore more complex real world environment, which requires longer-range credit assignment. 6. Scaling To Real World Websites In this section we will investigate scaling the Agent Q framework to real use cases on live websites, in particular bookings on OpenTable. We carried out initial experiments with the xLAM-v0.1-r model, which proved to weak for the task achieving an initial success rate of 0.0%. Instead we shifted to the LLaMa 70B Instruct model, which was able to achive some non-trivial initial success. 6.1. The OpenTable Environment In OpenTable, the agent is tasked with booking a restaurant reservation for a user. The agent must find a restaurant page on the OpenTable site, look for a reservation at a certain date and time, choose 12Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents Figure5: At the end of a trajectory, a GPT-4-V evaluator is called to provide feedback on the agent’s performance given the final observation and action history to determine the success score. The model is prompted with a condensed execution history of the trajectory and the screenshot of the final state. The success metric is a binary 0/1 value. seatingoptionsthatalignwithauser’spreferenceandsubmittheusercontactinformationtocomplete the task\n",
      "\n",
      "Top 2 matching chunk (Score: 0.5134):\n",
      "to sparsity of environment rewards. Indeed even a small mistake across the trajectory can cause the final agent output to be wrong, creating significant credit assignment problems. To overcome this, we use AI feedback (Bai et al., 2022) and self-criticism (Yuan et al., 2024) to further prompt the LLM to provide self-evaluation feedback at each node, which serves as intermediate reward and helps guide the search steps. This meaningfully improves the final agent success rate, but requires significant online interactions and moreover the capability to rollback actions, which is not always possible in online realistic settings. Such online autonomous search with little supervision on the web can result in a weak or unsafe agent which can make many errors, resulting in risky behaviors in sensitive online settings like bank transfers and sensitive information sharing. To correct this, we use the traces generated by the search process to improve capabilities of the model by learning from both the successful and unsuccessful trajectories with offline reinforcement learning, utilizing the Direct Preference Optimization (DPO) algorithm. We create preferences over different branches at the node level, which are scored using a mixture of the AI process feedback rewards and the final success rate of the explored branch. We evaluate our approach on the simulated WebShop benchmark (Yao et al., 2022)—a simulated e-commerce platform—as well as a real-world reservations booking website. We utilize LLaMa 3-70B as the base model in our experiments. In the WebShop environment, our approach consistently outperforms behavior cloning and reinforcement learning fine-tuned baselines, and beats average human performance when equipped with the capability to do online search. Inourreal-worldbookingexperiments, usingour Agent Q frameworkweimprovethemodelzero-shot absolute success rate from 18.6%to81.7%(a340% relative increase ), outperforming GPT-4’s performance after a single day of autonomous data collection. When we equip Agent Q with online search capability, our absolute success further improves to 95.4%. We believe that our approach represents a significant step forward in the development of autonomous web agents through it’s search and self-critique capabilities, setting a new benchmark for reliable multi-step decision-making in interactive settings. 2. Related Work Our work touches on a large number of research directions around agent design, self-improvement, reasoning and reinforcement learning. We include a short overview of related works from those various fields below. 2.1. Guided Search for Reasoning and Planning The latest generation of Large Language Models (LLMs) have demonstrated promising emerging properties around reasoning and planning. Moreover such behaviours can be directly elicited from strong models only using simple prompting techniques (Kojima et al., 2022; Qiao et al., 2023; Wei et al., 2022). These have also become an integral part of agentic design (Yao et al., 2023b; Zhang et al., 2024c), which we also utilize for our approach. Another emerging research direction is based around step-by-step verifiers or “Process Reward Models” (Lightman et al., 2023; Uesato et al., 2022), specifically for mathematical reasoning. These have shown to improve performance beyond purely outcome-based training, however they require a large amount of human effort to label individual steps. Some recent approaches have proposed self-supervised methods for step-level supervision (Hwang et al., 2024; Setlur et al., 2024a; Wang et al., 2024b). A number of concurrent works (Tian et al., 2024; Xie et al., 2024; Zhang et al., 2024e) have further explored tree-based search approaches 3Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents (Yao et al., 2023a) in combination with DPO (Rafailov et al., 2023) training for math-based reasoning. These algorithms optimize actions at the node level, using different branches produced by the search algorithm to create preference pairs. Our approach shares similarities to the self-supervised search proposed in (Yao et al., 2023a) with a combination of AI-based feedback (Bai et al., 2022; Yuan et al., 2024) to guide intermediate search steps, but we are the first to scale this a realistic agent setting. Similar approaches were proposed in (Hao et al., 2023; Zhou et al., 2024a), and other works (Koh et al., 2024); however these works only use the base model’s zero-shot capability and do not train it further. Moreover they are only evaluated on simulated environments. Beyond the search stage, our work further adopts the training methodology of (Tian et al., 2024; Xie et al., 2024; Zhang et al., 2024e), which significantly boosts our agent’s zero-shot capabilities. 2.2. Web Agents The strength and capabilities of recent pretrained Large Language (Vision) Models LL(V)Ms has significantly boosted progress in developing autonomous web-agents. Improved code understanding and long context have allowed agents to represent environment state and action space with document object model (DOM) allowing for deployment in complex and realistic domains. Moreover strong reasoning (Yao et al., 2023b) and planning (Liu et al., 2023; Zhang et al., 2024c) capabilities have also led to the development of a number of promising agents (Deng et al., 2023; Gur et al., 2024; Hong et al., 2023; Zhang and Zhang, 2023; Zhou et al., 2024b). Beyond using LL(V)Ms as plug-and-play planners/policies, recent works have sought to improve agentic-specific performance. Examples include online exploration (Zhang et al., 2024a), planning (Zhang et al., 2024b), error-correction (Wang et al., 2024a), and self- (Wu et al., 2024) or AI-critique (He et al., 2024; Pan et al., 2024). However, with small exceptions (Nakano et al., 2022) (which is still limited in scope) these agents mostly provide a framework around a strong pre-existing model like GPT4-V or deploy limited fine-tuning and adaptation. In this work we show that model training is crucial for continuous improvement. We combine a planning and reasoning agent with MCTS inference-time search and AI self-critique for self-supervised data collection, which we then use for RL type training. 2.3. Reinforcement Learning for LLMs and Agents ReinforcementLearninghasbecomeasignificantcomponentoftrainingmoderngenerativeAIsystems (Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023). Classical approaches have deployed the PPO algorithm (Schulman et al., 2017)—or similar policy-gradient based methods—and have even been scaled to autonomous web search agents (Nakano et al., 2022) as well as embodied applications with vision-language models (Zhai et al., 2024) (in simulation). However, these algorithms are challenging due to\n",
      "\n",
      "Top 3 matching chunk (Score: 0.4966):\n",
      "action. 3.2. Fine-Tuning Language Models From Feedback Classical approaches to RLHF in foundation models Ouyang et al. (2022); Stiennon et al. (2022) use the model as a policy 𝜋𝜃and optimize an objective of the form: Ea∼𝜋𝜃(a|h)[𝑟(a,h)]−𝛽D𝐾𝐿[𝜋𝜃(a|h)||𝜋ref(a|h)] (2) 6Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents where 𝜋refis some reference policy (usually the initial model). The goal of this formulation is to optimize some target objective (expressed by the reward 𝑟(a,h)) while preventing out-of-distribution drift. This objective can be extended to multi-step agentic problems, where the model interacts with an external environment envsuch as in Nakano et al. (2021) which focuses on information retrieval using web navigation. In this case we use an objective of the kind E𝜋𝜃,env[︃∑︁ 𝑡𝑟(a𝑡,h𝑡)]−𝛽D𝐾𝐿[𝜋𝜃(a𝑡)|h𝑡)||𝜋ref(a𝑡|h𝑡)]]︃ (3) Classical RLHF has used policy gradient type of algorithms, such as PPO Schulman et al. (2017), however, they are complex and require online data, which can be costly/dangerous to collect au- tonomously in the agent setting. While PPO has shown some success in prior web agent applications Nakano et al. (2021). The issues above largely make the approach not practical for general web tasks, beyond information retrieval. In this work we utilize some recent alternatives, outlined below. 3.2.1. Reinforced Fine-Tuning Reinforced fine-tuning (RFT) algorithms Gulcehre et al. (2023); Singh et al. (2024); Yuan et al. (2023); Zelikman et al. (2022) have grown in popularity due to their simplicity and scalability. These methods aggregate data and filter out the sub-optimal samples based on some reward model or a verifier to construct a growing dataset of high-quality trajectories 𝒟. Given this dataset and a parameterized model 𝜋𝜃we can carry out standard supervised fine-tuning (SFT): ℒ(𝜋𝜃,𝒟) =−E𝒟[︃𝑇∑︁ 𝑡=1log𝜋𝜃(a𝑡|h𝑡)]︃ (4) In this objective the divergence penalty is only applied implicitly by limiting the number of training rounds. While simple and relatively successful, empirically these methods tend to under-perform standard RL and alternatives Dubois et al. (2024); Setlur et al. (2024b); Tajwar et al. (2024) in the text generation domain, particularly in reasoning. We largely observe similar empirical results, and we use these methods mostly as baselines to build intuition. 3.2.2. Direct Preference Optimization Direct Preference Optimization (DPO) Rafailov et al. (2023) is an offline RL Levine et al. (2020) alternative to the classical RLHF optimization pipeline. It is a suitable algorithm for agent fine-tuning, as it can use fully offline data and does not require online rollouts. The original formulation in the pure text generation setting considers feedback of pairwise comparisons (h,a𝑤,a𝑙), where sis a single prompt and a𝑤anda𝑙are two responses with a𝑤≻a𝑙indicating that a𝑤is preferred over a𝑙. The DPO objective then minimizes the following loss: ℒDPO(𝜋𝜃;𝒟) =−E(h,a𝑤,a𝑙)∼𝒟[︂ log𝜎(︂(︂ 𝛽log𝜋𝜃(a𝑤|h𝑤) 𝜋ref(a𝑤|h𝑤))︂ −(︂ 𝛽log𝜋𝜃(a𝑙|h𝑙) 𝜋ref(a𝑙|h𝑙))︂)︂]︂ (5) While the algorithm was developed in a bandit setting Hejna et al. (2024); Rafailov et al. (2024) have extended it to multi-turn settings with preferences over over trajectories. In our setting, we can directly utilize this objective as: ℒT-DPO(𝜋𝜃;𝒟) =−E(𝜏𝑤,𝜏𝑙)∼𝒟⎡ ⎣log𝜎⎛ ⎝⎛ ⎝|𝜏𝑤|∑︁ 𝑡=0𝛽log𝜋𝜃(a𝑤 𝑡|h𝑤 𝑡) 𝜋ref(a𝑤 𝑡|h𝑤 𝑡)⎞ ⎠−⎛ ⎝|𝜏𝑙|∑︁ 𝑡=0𝛽log𝜋𝜃(a𝑙 𝑡|h𝑙 𝑡) 𝜋ref(a𝑙 𝑡|h𝑙 𝑡)⎞ ⎠⎞ ⎠⎤ ⎦(6) 7Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents Figure3: Success rate of different approaches on the WebShop Yao et al. (2022) task. All models are based on xLAM-v0.1-r Zhang et al. (2024c). RFT and DPO over xLAM-v0.1-r demonstrate improvements in performance from 28.6% to 31.3% and 37.5% respectively. However, these methods still lag behind average human performance of 50.0%. Our approach, Agent Q + MCTS achieves a significant gain (76.57% relative improvement) over the base model, outperforming average human performance on WebShop with a success rate of 50.5%. One bottleneck for the practical deployment of the algorithm is the need for a reference model 𝜋ref during optimization, which requires more computational resources. Instead in our settings, we slightly modify the algorithm using an off-policy replay buffer, which aggregates trajectory data, as well as likelihoods of the generated actions. During the optimization step, we sample tuples of trajectories and the corresponding likelihoods under the data generation (reference) density, which eliminates the need for a separate reference model. 4. Preliminary Approach With Outcome Supervision In this section we will outline preliminary experimental results, which will build the base understand- ing for our further experiments. We use the AgentOhana xLAM-v0.1-r model Zhang et al. (2024c), which is a fine-tune of a pre-trained Mixtral-8x7B-Instruct-v0.1 model Jiang et al. (2024) on a mix of agentic applications, including WebShop SFT data. We also incorporate the same agent configuration 1specified by the AgentLite Liu et al. (2024) work to ensure a fair comparison between our fine-tuned model and the xLAM base model performance. We evaluate all approaches on the WebShop environ- ment Yao et al. (2022), where the agent needs to find particular products by browsing a simulated web shop. The environment comes with a set of 12,087 pre-defined tasks (corresponding to specific products to find), which we split into a train set of 11,000 tasks, which we use for further agent fine-tuning and a set of 1,087 held-out tasks, which we use for zero-shot evaluation. We show success rates (exact product match) for different approaches in Fig. 3. The base xLAM-v0.1-r model achieves success rate of 28.6% on the test tasks. All other methods are based on outcome-based supervision 1https://github.com/SalesforceAIResearch/xLAM 8Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents only, depending on whether a particular attempt was successful or not. We see that further RFT training, using a STaR-like algorithm Zelikman et al. (2022) on the trajectory level, as outlined in Sec. 3.2.1, achieves success rate of 31.3%, which is a small improvements of 2.7% over the initial model. This is not surprising since the base model is already trained as an agent on the environment with supervised fine-tuning on demonstrations. Our next experiment fine-tunes the base model using the trajectory-level DPO algorithm, as outlined in Eq. 6 in Sec. 3.2.2 using successful trajectories as preferred over failed ones. This approach also uses only outcome-level supervision, but unlike the RFT baseline can utilize failed trajectories as well, which improves the agent\n",
      "\n",
      "The text describes a methodology for fine-tuning an autonomous AI agent using Reinforced Fine-Tuning (RFT) and Direct Preference Optimization (DPO). Here's a breakdown of the methodology:\n",
      "\n",
      "**Background**: The goal is to improve the performance of an existing AI model, referred to as \"xLAM-v0.1-r\", which has been trained on a specific task but performs poorly.\n",
      "\n",
      "**Methodology**:\n",
      "\n",
      "1. **Data collection**: A dataset of high-quality trajectories is collected by aggregating data and filtering out sub-optimal samples based on a reward model or verifier.\n",
      "2. **Reinforced Fine-Tuning (RFT)**: The collected dataset is used to fine-tune the existing AI model using supervised fine-tuning (SFT) with a parameterized model πθ. This involves maximizing the log-likelihood of the model's predictions given the data.\n",
      "3. **Direct Preference Optimization (DPO)**: An offline RL evaluation method that uses successful trajectories as preferred over failed ones to optimize the agent's performance.\n",
      "\n",
      "**Experiments**:\n",
      "\n",
      "1. **RFT**: The base model is fine-tuned using RFT with a STaR-like algorithm on the trajectory level, which achieves a small improvement of 2.7% in success rate.\n",
      "2. **DPO**: The base model is fine-tuned using trajectory-level DPO algorithm (Equation6), which uses only outcome-based supervision and utilizes failed trajectories as well as successful ones, which improves agent performance by 16.9%.\n",
      "\n",
      "**Key Takeaways**:\n",
      "\n",
      "* RFT algorithms are simple and scalable but tend to underperform standard RL methods.\n",
      "* DPO achieves better results than the baseline model.\n",
      "\n",
      "Overall, this methodology aims to improve the performance of an existing AI model using data-driven approaches like DPO on a specific task, referred as Advanced Reasoning, outperforms both RFT and DPO baselines.\n",
      "--------------------------------------------------\n",
      "Question: Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. \n",
      "Top 1 matching chunk (Score: 0.7717):\n",
      "Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models, 2023. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476–15488, 2022. Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, YannLeCun,YiMa,andSergeyLevine. Fine-tuninglargevision-languagemodelsasdecision-making agents via reinforcement learning, 2024. Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, and Saravan Rajmohan. Ufo: A ui-focused agent for windows os interaction. arXiv, 2024a. URL https://api.semanticscholar.org/CorpusID:267211622 . Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users, 2023. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Ap- pagent: Multimodalagentsassmartphoneusers. arXiv,2024b. URL https://api.semanticscholar. org/CorpusID:262053313 . Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, and Caiming Xiong. Agentohana: Design unified data and training pipeline for effective agent learning, 2024c. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges, 2024d. XuanZhang,ChaoDu,TianyuPang,QianLiu,WeiGao,andMinLin. Chainofpreferenceoptimization: Improving chain-of-thought reasoning in llms, 2024e. XuanZhang,ChaoDu,TianyuPang,QianLiu,WeiGao,andMinLin. Chainofpreferenceoptimization: Improving chain-of-thought reasoning in llms, 2024f. URL https://arxiv.org/abs/2406.09136 . Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. ArXiv, 2023. URL https://api.semanticscholar.org/CorpusID:262053313 . Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models, 2024a. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environ- ment for building autonomous agents. In ICLR, 2024b. Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl, 2024c. 22\n",
      "\n",
      "Top 2 matching chunk (Score: 0.6350):\n",
      "to sparsity of environment rewards. Indeed even a small mistake across the trajectory can cause the final agent output to be wrong, creating significant credit assignment problems. To overcome this, we use AI feedback (Bai et al., 2022) and self-criticism (Yuan et al., 2024) to further prompt the LLM to provide self-evaluation feedback at each node, which serves as intermediate reward and helps guide the search steps. This meaningfully improves the final agent success rate, but requires significant online interactions and moreover the capability to rollback actions, which is not always possible in online realistic settings. Such online autonomous search with little supervision on the web can result in a weak or unsafe agent which can make many errors, resulting in risky behaviors in sensitive online settings like bank transfers and sensitive information sharing. To correct this, we use the traces generated by the search process to improve capabilities of the model by learning from both the successful and unsuccessful trajectories with offline reinforcement learning, utilizing the Direct Preference Optimization (DPO) algorithm. We create preferences over different branches at the node level, which are scored using a mixture of the AI process feedback rewards and the final success rate of the explored branch. We evaluate our approach on the simulated WebShop benchmark (Yao et al., 2022)—a simulated e-commerce platform—as well as a real-world reservations booking website. We utilize LLaMa 3-70B as the base model in our experiments. In the WebShop environment, our approach consistently outperforms behavior cloning and reinforcement learning fine-tuned baselines, and beats average human performance when equipped with the capability to do online search. Inourreal-worldbookingexperiments, usingour Agent Q frameworkweimprovethemodelzero-shot absolute success rate from 18.6%to81.7%(a340% relative increase ), outperforming GPT-4’s performance after a single day of autonomous data collection. When we equip Agent Q with online search capability, our absolute success further improves to 95.4%. We believe that our approach represents a significant step forward in the development of autonomous web agents through it’s search and self-critique capabilities, setting a new benchmark for reliable multi-step decision-making in interactive settings. 2. Related Work Our work touches on a large number of research directions around agent design, self-improvement, reasoning and reinforcement learning. We include a short overview of related works from those various fields below. 2.1. Guided Search for Reasoning and Planning The latest generation of Large Language Models (LLMs) have demonstrated promising emerging properties around reasoning and planning. Moreover such behaviours can be directly elicited from strong models only using simple prompting techniques (Kojima et al., 2022; Qiao et al., 2023; Wei et al., 2022). These have also become an integral part of agentic design (Yao et al., 2023b; Zhang et al., 2024c), which we also utilize for our approach. Another emerging research direction is based around step-by-step verifiers or “Process Reward Models” (Lightman et al., 2023; Uesato et al., 2022), specifically for mathematical reasoning. These have shown to improve performance beyond purely outcome-based training, however they require a large amount of human effort to label individual steps. Some recent approaches have proposed self-supervised methods for step-level supervision (Hwang et al., 2024; Setlur et al., 2024a; Wang et al., 2024b). A number of concurrent works (Tian et al., 2024; Xie et al., 2024; Zhang et al., 2024e) have further explored tree-based search approaches 3Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents (Yao et al., 2023a) in combination with DPO (Rafailov et al., 2023) training for math-based reasoning. These algorithms optimize actions at the node level, using different branches produced by the search algorithm to create preference pairs. Our approach shares similarities to the self-supervised search proposed in (Yao et al., 2023a) with a combination of AI-based feedback (Bai et al., 2022; Yuan et al., 2024) to guide intermediate search steps, but we are the first to scale this a realistic agent setting. Similar approaches were proposed in (Hao et al., 2023; Zhou et al., 2024a), and other works (Koh et al., 2024); however these works only use the base model’s zero-shot capability and do not train it further. Moreover they are only evaluated on simulated environments. Beyond the search stage, our work further adopts the training methodology of (Tian et al., 2024; Xie et al., 2024; Zhang et al., 2024e), which significantly boosts our agent’s zero-shot capabilities. 2.2. Web Agents The strength and capabilities of recent pretrained Large Language (Vision) Models LL(V)Ms has significantly boosted progress in developing autonomous web-agents. Improved code understanding and long context have allowed agents to represent environment state and action space with document object model (DOM) allowing for deployment in complex and realistic domains. Moreover strong reasoning (Yao et al., 2023b) and planning (Liu et al., 2023; Zhang et al., 2024c) capabilities have also led to the development of a number of promising agents (Deng et al., 2023; Gur et al., 2024; Hong et al., 2023; Zhang and Zhang, 2023; Zhou et al., 2024b). Beyond using LL(V)Ms as plug-and-play planners/policies, recent works have sought to improve agentic-specific performance. Examples include online exploration (Zhang et al., 2024a), planning (Zhang et al., 2024b), error-correction (Wang et al., 2024a), and self- (Wu et al., 2024) or AI-critique (He et al., 2024; Pan et al., 2024). However, with small exceptions (Nakano et al., 2022) (which is still limited in scope) these agents mostly provide a framework around a strong pre-existing model like GPT4-V or deploy limited fine-tuning and adaptation. In this work we show that model training is crucial for continuous improvement. We combine a planning and reasoning agent with MCTS inference-time search and AI self-critique for self-supervised data collection, which we then use for RL type training. 2.3. Reinforcement Learning for LLMs and Agents ReinforcementLearninghasbecomeasignificantcomponentoftrainingmoderngenerativeAIsystems (Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023). Classical approaches have deployed the PPO algorithm (Schulman et al., 2017)—or similar policy-gradient based methods—and have even been scaled to autonomous web search agents (Nakano et al., 2022) as well as embodied applications with vision-language models (Zhai et al., 2024) (in simulation). However, these algorithms are challenging due to\n",
      "\n",
      "Top 3 matching chunk (Score: 0.6344):\n",
      "Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents Pranav Putta1, Edmund Mills1, Naman Garg1, Sumeet Motwani1, Chelsea Finn2, Divyansh Garg1and Rafael Rafailov2 1The AGI Company (MultiOn),2Stanford University LargeLanguageModels(LLMs)haveshownremarkablecapabilitiesinnaturallanguagetasksrequiringcomplex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous agentcapabilitiesneededtoperformcomplexdecision-makingindynamicsettingslikewebnavigation. Previous attempts to bridge this gap through supervised fine-tuning on curated expert demonstrations often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome these challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm. Our method allows LLM agents to learn effectively from both successfulandunsuccessfultrajectories,therebyimprovingtheirgeneralizationincomplex,multi-stepreasoning tasks. We validate our approach in the WebShop environment, a simulated e-commerce platform—where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search. In real-world booking scenarios, our methodology boosts Llama-3 70B model’s zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase ) after a single day of data collection and further to 95.4%with online search. We believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings. 1. Introduction The recent advances in Large Language Models (LLMs) represent a significant leap in artificial intelligence. Frontier models like ChatGPT (John Schulman et al., 2022), Gemini (Anil et al., 2023), Opus (Anthropic, 2024), and LLaMA-3 (Touvron et al., 2023) demonstrate promising reasoning capabilities that approach average human performance in a number of domains. These breakthroughs have extended the utility of LLMs from traditional chat and text-based applications to more dynamic, agentic roles, in which they do not just generate text but can take actions autonomously in a number of environments including code and software engineering (Holt et al., 2024; Jimenez et al., 2024; Yang et al., 2024; Zhang et al., 2024d), device control (Chen and Li, 2024; Wang et al., 2024a; Zhang et al., 2023) and web applications (Deng et al., 2023; Gur et al., 2024; Hong et al., 2023; Lai et al., 2024a; Zhou et al., 2024b) among others. However, despite these advancements, significant challenges persist: LLMs still struggle to generalize effectively in interactive, multi-step environments, since they are not native trained for such applications . This is true, even for some of the strongest models of the current generation, such as GPT-4 (Achiam et al., 2023). A growing literature on agentic formulation seeks to address these issues; however these works mostly focus on building frameworks around prompt-based learning on existing models or limited fine-tuning on static datasets, and are thus limited by the base models’ reasoning and decision making capabilities. Reasoning and planning have indeed been highlighted as core challenges for current LLMs. Since the seminal work on chain-of-thought reasoning (Wei et al., 2022), significant efforts have been made to improve these capabilities via prompt-based strategies (Kojima et al., 2022; Qiao et al., 2023; Corresponding author(s): div@multion.ai, rafailov@stanford.eduarXiv:2408.07199v1 [cs.AI] 13 Aug 2024Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents Figure1: We use Monte Carlo Tree Search (MCTS) to guide trajectory collection and iteratively improve model performance using direct preference optimization (DPO). We begin on the left by sampling a user query from the list of tasks in the dataset. We iteratively expand the search tree using UCB1 as a heuristic to balance exploration and exploitation of different actions. We store the accumulated reward obtained for each node in the tree, where in this image darker green indicates higherrewardanddarkerredindicateslowerreward. Toconstructthepreferencedataset,wecompute a weighted score of the MCTS average Q-value and score generated by a feedback language model to construct contrastive pairs for DPO. The policy is optimized and can be iteratively improved. Wang et al., 2023; Yao et al., 2023a). While successful, these approaches are still bounded by the base model’s performance. Another direction of research has explored fine-tuning approaches (Pang et al., 2024; Zelikman et al., 2022), and more recently combining them with inference-time search prompting (Yao et al., 2023a) to produce fine-grained feedback. Concurrent works (Hwang et al., 2024; Tian et al., 2024; Xie et al., 2024; Zhang et al., 2024e) utilize the traces produced by search algorithms and combine them with optimization approaches (Rafailov et al., 2023; Zelikman et al., 2022) to achieve significant boost in capabilities, especially in mathematics problem solving and code generation. In this work we explore improving planning and reasoning capabilities of a web agent, which interacts with a real world website. Our goal is to design an approach that allows the agent to improve with autonomous experience and limited supervision. Indeed, prior works (Masterman et al., 2024; Sumers et al., 2024; Yao et al., 2023b; Zhang et al., 2024c) have shown strong reasoning to be critical for performance of autonomous agents, where challenges are even greater than during text generation, as the model needs to further understand how its actions affect its environment. Towards this goal, we introduce Agent Q —a novel approach that combines several key concepts in reasoning, search, self-critique and reinforcement learning. Our method takes inspiration from Sutton’s The Bitter Lesson on the power of general purpose methods that continue to scale with increased computation, showing the significant benefits of combining searchandlearning . Inspired by the success of search-based methods in prior game-playing settings (Brown and Sandholm, 2019; Gray et al., 2021; Silver et al., 2017a) and mathematical reasoning (Besta et al., 2024; Yao et al., 2023a), we deploy a Monte Carlo Tree Search (MCTS) based search routine over web pages to guide agent exploration. Given the complexity of the environment, we use a base LLM for sampling 2Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents possible rationales and web actions to explore. While this simple search-strategy shows a meaningful improvement in the success rate, it still struggles on long horizon tasks due\n",
      "\n",
      "This text appears to be a research paper or article on the topic of \"Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\". I'll break down the content into sections for easier understanding.\n",
      "\n",
      "**Section 1: Introduction**\n",
      "\n",
      "* The authors discuss recent advances in Large Language Models (LLMs) that demonstrate promising reasoning capabilities, approaching average human performance in various domains.\n",
      "* These breakthroughs, autonomous agents still struggle with complex decision-making in dynamic environments, especially when actions affect future outcomes.\n",
      "* Existing methods mostly fine-tuned on static datasets or using prompt-based learning around existing models which are still limited by their reasoning and decision making capabilities.\n",
      "\n",
      "**Section 2: Background**\n",
      "\n",
      "* The authors discuss the challenges of scaling up LLMs to more complex tasks such as code generation, device control, and web applications.\n",
      "* They highlight that while current methods can achieve success in specific domains, they struggle to generalize across different environments and scenarios.\n",
      "* This is due to the static training data used for fine-tuning, which does not capture the dynamic nature of real-world interactions.\n",
      "\n",
      "**Section 3: Methodology**\n",
      "\n",
      "* The authors propose a novel approach called Agent Q, which combines several key concepts in reasoning, search, and learning.\n",
      "* They use Monte Carlo Tree Search (MCTS) to guide trajectory collection and iteratively improve model performance using direct preference optimization (DPO).\n",
      "* DPO allows the agent to learn from both successful and unsuccessful trajectories, improving generalization.\n",
      "\n",
      "**Section 4: Validation**\n",
      "\n",
      "* The authors validate their approach in the WebShop environment, a simulated e-commerce platform.\n",
      "* They show that Agent Q outperforms behavior cloning and reinforced fine-tuning baseline methods and beats average human performance when equipped with online search capabilities.\n",
      "\n",
      "**Section 5: Real-world Results**\n",
      "\n",
      "* The authors demonstrate real-world results using the LLaMA-3 model in booking scenarios.\n",
      "* They achieve a significant leap forward, boosting zero-shot performance from 18.6% to 95.4% success rate after data collection and reinforcement learning.\n",
      "\n",
      "Overall, this research paper explores the challenges of scaling up LLMs for complex tasks and proposes a novel approach called Agent Q that combines reasoning, search, and learning concepts to achieve significant breakthroughs in autonomous agents' performance.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "filename = \"./media/AgentQ.pdf\"\n",
    "questions = [\n",
    "    \"What is the main topic discussed in the document?\",\n",
    "    \"Can you explain the methodology used?\",\n",
    "    \"Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. \"\n",
    "]\n",
    "\n",
    "# Get the answers\n",
    "answers = rag_ollama(filename, questions, chunk_size=1000, top_n=3)\n",
    "\n",
    "for answer in answers:\n",
    "    print(f\"Question: {answer['question']}\")\n",
    "    for idx, (chunk, similarity) in enumerate(answer['top_chunks'], 1):\n",
    "        print(f\"Top {idx} matching chunk (Score: {similarity:.4f}):\\n{chunk}\\n\")\n",
    "    \n",
    "    llm_response = generate(answer['question'], answer['top_chunks'])\n",
    "    print(llm_response[\"message\"][\"content\"])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"mongoadmin\"  \n",
    "password = \"super-save-password\"  \n",
    "host = \"localhost\"          \n",
    "port = 27017                \n",
    "database_name = \"vector_store\"  \n",
    "\n",
    "# Construct the MongoDB URI with authentication\n",
    "connection_string = f\"mongodb://{username}:{password}@{host}:{port}\"\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(connection_string, connect=True)\n",
    "database = client.get_database(database_name)\n",
    "collection = database.get_collection(\"lecture_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo.collection\n",
    "\n",
    "def insert_embedding(embedding: list[float], text: str, collection: pymongo.collection.Collection):\n",
    "\n",
    "    # Document to insert\n",
    "    document = {\n",
    "        \"vector\": embedding,  # The vector field\n",
    "        \"magnitude\": sum(x**2 for x in embedding)**0.5,  # Optional: precompute the vector's magnitude\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "    # Insert the document\n",
    "    result = collection.insert_one(document)\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Inserted document with ID: {result.inserted_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 6738af3bcfefad7a72de04ec\n",
      "Inserted document with ID: 6738af3bcfefad7a72de04ed\n",
      "Inserted document with ID: 6738af3bcfefad7a72de04ee\n",
      "Inserted document with ID: 6738af3bcfefad7a72de04ef\n",
      "Inserted document with ID: 6738af3bcfefad7a72de04f0\n",
      "Inserted document with ID: 6738af3ccfefad7a72de04f1\n",
      "Inserted document with ID: 6738af3ccfefad7a72de04f2\n",
      "Inserted document with ID: 6738af3ccfefad7a72de04f3\n",
      "Inserted document with ID: 6738af3ccfefad7a72de04f4\n",
      "Inserted document with ID: 6738af3ccfefad7a72de04f5\n",
      "Inserted document with ID: 6738af3ccfefad7a72de04f6\n"
     ]
    }
   ],
   "source": [
    "filename = \"./media/AgentQ.pdf\"\n",
    "chunk_size = 1000\n",
    "model_name: str=\"nomic-embed-text:latest\"\n",
    "\n",
    "pdf_text = extract_text_from_pdf(filename)\n",
    "    \n",
    "    # Step 2: Chunk the PDF text\n",
    "pdf_chunks = chunk_text(pdf_text, chunk_size)\n",
    "\n",
    "# Step 3: Embed the PDF chunks\n",
    "chunk_embeddings = ollama.embed(model=model_name, input=pdf_chunks)[\"embeddings\"]\n",
    "\n",
    "for idx, chunk_embedding in enumerate(chunk_embeddings):\n",
    "    insert_embedding(chunk_embedding, pdf_chunks[idx], collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_chunks_from_mongo(question: str, collection: pymongo.collection.Collection, model_name: str = \"nomic-embed-text:latest\", k_entries: int = 5):\n",
    "    question_embedding = ollama.embed(model=model_name, input=question)[\"embeddings\"][0]\n",
    "    query_magnitude = np.sqrt(np.sum(np.square(question_embedding)))\n",
    "    \n",
    "    pipeline =  [\n",
    "        {\n",
    "            # Compute the dot product\n",
    "            \"$addFields\": {\n",
    "                \"dot_product\": {\n",
    "                    \"$sum\": {\n",
    "                        \"$map\": {\n",
    "                            \"input\": {\"$range\": [0, {\"$size\": \"$vector\"}]},  # Iterate over indices\n",
    "                            \"as\": \"index\",\n",
    "                            \"in\": {\n",
    "                                \"$multiply\": [\n",
    "                                    {\"$arrayElemAt\": [\"$vector\", \"$$index\"]},\n",
    "                                    {\"$arrayElemAt\": [question_embedding, \"$$index\"]}\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            # Compute magnitude of stored vectors dynamically if not precomputed\n",
    "            \"$addFields\": {\n",
    "                \"vector_magnitude\": {\n",
    "                    \"$sqrt\": {\n",
    "                        \"$sum\": {\n",
    "                            \"$map\": {\n",
    "                                \"input\": \"$vector\",\n",
    "                                \"as\": \"x\",\n",
    "                                \"in\": {\"$multiply\": [\"$$x\", \"$$x\"]}\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            # Compute cosine similarity\n",
    "            \"$addFields\": {\n",
    "                \"cosine_similarity\": {\n",
    "                    \"$cond\": {\n",
    "                        \"if\": {\"$and\": [{\"$gt\": [\"$vector_magnitude\", 0]}, {\"$gt\": [query_magnitude, 0]}]},\n",
    "                        \"then\": {\n",
    "                            \"$divide\": [\n",
    "                                \"$dot_product\",\n",
    "                                {\"$multiply\": [\"$vector_magnitude\", query_magnitude]}\n",
    "                            ]\n",
    "                        },\n",
    "                        \"else\": 0\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            # Sort by cosine similarity in descending order\n",
    "            \"$sort\": {\"cosine_similarity\": -1}\n",
    "        },\n",
    "        {\n",
    "            # Limit the number of results\n",
    "            \"$limit\": k_entries\n",
    "        },\n",
    "        {\n",
    "            # Project the fields you want in the output\n",
    "            \"$project\": {\"_id\": 1, \"cosine_similarity\": 1, \"text\": 1}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = list(collection.aggregate(pipeline))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who wrote the paper 'attention is all you need'?\"\n",
    "\n",
    "chunks = fetch_chunks_from_mongo(question, collection)\n",
    "prepped_chunks = [(x[\"text\"], x[\"cosine_similarity\"]) for x in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From DB Pipeline\n",
      "is auto-re score: 0.5308237258832028\n",
      "for LSTM n score: 0.5285202409229166\n",
      "and observ score: 0.497049528147839\n",
      "the layers score: 0.47391614027930395\n",
      "English-Fr score: 0.4373282603842479\n",
      "--------------------------------------------------\n",
      "\n",
      "From Script\n",
      "Zhou, and  score: 0.4321475263448718\n",
      "Agent Q: A score: 0.41545025954904324\n",
      "suitable.  score: 0.41537031717508704\n",
      "successful score: 0.40572579791169966\n",
      "tree from  score: 0.3973562382018639\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"From DB Pipeline\")\n",
    "for chunk, score in prepped_chunks:\n",
    "    print(chunk[:10], f\"score: {score}\")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nFrom Script\")\n",
    "for answer in rag_ollama(filename, [question], chunk_size=1000, top_n=5):\n",
    "    for idx, (chunk, score) in enumerate(answer['top_chunks'], 1):\n",
    "        print(chunk[:10], f\"score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of the paper \"Attention Is All You Need\" are:\n",
      "\n",
      "* Vaswani, Ashish\n",
      "* Shazeer, Noam\n",
      "* Parmar, Niki\n",
      "* Uszkoreit, Jakob\n",
      "* Jones, Llion\n",
      "* Gomez, Ajay\n",
      "* Kaiser, Luke\n",
      "* Polosukhin, Ilya\n",
      "\n",
      "This paper was published in 2017 and introduced the Transformer architecture, which is the core of the model described in the text you provided.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who wrote the paper 'attention is all you need'?\"\n",
    "\n",
    "llm_response = generate(question, [(x[\"text\"], x[\"cosine_similarity\"]) for x in fetch_chunks_from_mongo(question, collection)])\n",
    "print(llm_response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
