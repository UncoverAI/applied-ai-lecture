{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama can be used with GGML or GGUF. However GGUF is the successor of GGML due to some constrains source: [medium](https://medium.com/@phillipgimmi/what-is-gguf-and-ggml-e364834d241c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "# ... Here could be some tuning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model\\\\tokenizer_config.json',\n",
       " 'model\\\\special_tokens_map.json',\n",
       " 'model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"model\")\n",
    "tokenizer.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n"
     ]
    }
   ],
   "source": [
    "# Create the directory move into it and clone llama.cpp there\n",
    "!mkdir dependencies && cd dependencies && git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\Development\\GitHub Repos\\UncoverAI\\applied-ai-lecture\\.venv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\Development\\GitHub Repos\\UncoverAI\\applied-ai-lecture\\.venv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\Development\\GitHub Repos\\UncoverAI\\applied-ai-lecture\\.venv\\Lib\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.5.1+cu124 requires torch==2.5.1+cu124, but you have torch 2.2.2+cpu which is incompatible.\n",
      "torchvision 0.20.1+cu124 requires torch==2.5.1+cu124, but you have torch 2.2.2+cpu which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Collecting numpy~=1.26.4 (from -r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 1))\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/61.0 kB 320.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 61.0/61.0 kB 651.3 kB/s eta 0:00:00\n",
      "Collecting sentencepiece~=0.2.0 (from -r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 2))\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from -r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (4.47.0)\n",
      "Collecting gguf>=0.1.0 (from -r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 4))\n",
      "  Downloading gguf-0.13.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.0 (from -r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 5))\n",
      "  Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting torch~=2.2.1 (from -r dependencies/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3))\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp312-cp312-win_amd64.whl (200.7 MB)\n",
      "     ---------------------------------------- 0.0/200.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/200.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.2/200.7 MB 3.1 MB/s eta 0:01:04\n",
      "     ---------------------------------------- 0.4/200.7 MB 3.4 MB/s eta 0:00:59\n",
      "     ---------------------------------------- 0.9/200.7 MB 5.1 MB/s eta 0:00:39\n",
      "     ---------------------------------------- 1.5/200.7 MB 8.0 MB/s eta 0:00:25\n",
      "      -------------------------------------- 2.8/200.7 MB 10.5 MB/s eta 0:00:19\n",
      "      -------------------------------------- 3.9/200.7 MB 12.3 MB/s eta 0:00:16\n",
      "      -------------------------------------- 5.1/200.7 MB 14.1 MB/s eta 0:00:14\n",
      "     - ------------------------------------- 6.6/200.7 MB 16.2 MB/s eta 0:00:12\n",
      "     - ------------------------------------- 8.3/200.7 MB 18.3 MB/s eta 0:00:11\n",
      "     - ------------------------------------- 9.9/200.7 MB 19.9 MB/s eta 0:00:10\n",
      "     -- ----------------------------------- 11.7/200.7 MB 28.5 MB/s eta 0:00:07\n",
      "     -- ----------------------------------- 13.5/200.7 MB 34.4 MB/s eta 0:00:06\n",
      "     -- ----------------------------------- 15.4/200.7 MB 36.4 MB/s eta 0:00:06\n",
      "     --- ---------------------------------- 17.5/200.7 MB 38.5 MB/s eta 0:00:05\n",
      "     --- ---------------------------------- 19.7/200.7 MB 40.9 MB/s eta 0:00:05\n",
      "     ---- --------------------------------- 22.0/200.7 MB 43.5 MB/s eta 0:00:05\n",
      "     ---- --------------------------------- 24.4/200.7 MB 46.7 MB/s eta 0:00:04\n",
      "     ----- -------------------------------- 26.9/200.7 MB 50.1 MB/s eta 0:00:04\n",
      "     ----- -------------------------------- 29.3/200.7 MB 50.4 MB/s eta 0:00:04\n",
      "     ------ ------------------------------- 32.2/200.7 MB 54.7 MB/s eta 0:00:04\n",
      "     ------ ------------------------------- 35.2/200.7 MB 59.5 MB/s eta 0:00:03\n",
      "     ------- ------------------------------ 38.2/200.7 MB 59.5 MB/s eta 0:00:03\n",
      "     ------- ------------------------------ 41.6/200.7 MB 65.6 MB/s eta 0:00:03\n",
      "     -------- ----------------------------- 45.1/200.7 MB 72.6 MB/s eta 0:00:03\n",
      "     --------- ---------------------------- 48.8/200.7 MB 72.6 MB/s eta 0:00:03\n",
      "     ---------- --------------------------- 53.0/200.7 MB 81.8 MB/s eta 0:00:02\n",
      "     ---------- --------------------------- 57.3/200.7 MB 81.8 MB/s eta 0:00:02\n",
      "     ----------- -------------------------- 61.8/200.7 MB 93.9 MB/s eta 0:00:02\n",
      "     ------------ ------------------------- 66.1/200.7 MB 93.9 MB/s eta 0:00:02\n",
      "     ------------- ------------------------ 70.6/200.7 MB 93.9 MB/s eta 0:00:02\n",
      "     -------------- ----------------------- 75.3/200.7 MB 93.9 MB/s eta 0:00:02\n",
      "     -------------- ---------------------- 80.4/200.7 MB 108.8 MB/s eta 0:00:02\n",
      "     --------------- --------------------- 85.9/200.7 MB 108.8 MB/s eta 0:00:02\n",
      "     ---------------- -------------------- 91.3/200.7 MB 108.8 MB/s eta 0:00:02\n",
      "     ----------------- ------------------- 96.7/200.7 MB 110.0 MB/s eta 0:00:01\n",
      "     ------------------ ----------------- 102.1/200.7 MB 110.0 MB/s eta 0:00:01\n",
      "     ------------------- ---------------- 107.5/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     -------------------- --------------- 112.9/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------- -------------- 118.2/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ---------------------- ------------- 123.6/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ----------------------- ------------ 129.1/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ------------------------ ----------- 134.5/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ------------------------- ---------- 139.9/200.7 MB 110.0 MB/s eta 0:00:01\n",
      "     -------------------------- --------- 145.4/200.7 MB 110.0 MB/s eta 0:00:01\n",
      "     --------------------------- -------- 150.7/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------------- -------- 156.0/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ------- 161.4/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ------ 166.8/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ------------------------------ ----- 172.2/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ---- 177.5/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     -------------------------------- --- 182.9/200.7 MB 110.0 MB/s eta 0:00:01\n",
      "     --------------------------------- -- 188.2/200.7 MB 110.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- - 193.6/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     -----------------------------------  198.9/200.7 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  200.7/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     -----------------------------------  200.7/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     -----------------------------------  200.7/200.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- 200.7/200.7 MB 46.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (0.26.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from torch~=2.2.1->-r dependencies/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: sympy in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from torch~=2.2.1->-r dependencies/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: networkx in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from torch~=2.2.1->-r dependencies/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from torch~=2.2.1->-r dependencies/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from torch~=2.2.1->-r dependencies/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.10.0)\n",
      "Requirement already satisfied: colorama in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from jinja2->torch~=2.2.1->-r dependencies/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.45.1->-r dependencies/llama.cpp/requirements\\./requirements-convert_legacy_llama.txt (line 3)) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\development\\github repos\\uncoverai\\applied-ai-lecture\\.venv\\lib\\site-packages (from sympy->torch~=2.2.1->-r dependencies/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/15.5 MB 2.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.4/15.5 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.5/15.5 MB 46.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.8/15.5 MB 110.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 93.8 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 992.0/992.0 kB 61.4 MB/s eta 0:00:00\n",
      "Downloading gguf-0.13.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.0/75.0 kB ? eta 0:00:00\n",
      "Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 25.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, protobuf, numpy, torch, gguf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu124\n",
      "    Uninstalling torch-2.5.1+cu124:\n",
      "      Successfully uninstalled torch-2.5.1+cu124\n",
      "Successfully installed gguf-0.13.0 numpy-1.26.4 protobuf-4.25.5 sentencepiece-0.2.0 torch-2.2.2+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install -r dependencies/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> Q8_0, shape = {2048, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:converted\\gguf_model.gguf: n_tensors = 147, total_size = 1.3G\n",
      "\n",
      "Writing:   0%|          | 0.00/1.31G [00:00<?, ?byte/s]\n",
      "Writing:  21%|██▏       | 279M/1.31G [00:01<00:07, 140Mbyte/s]\n",
      "Writing:  23%|██▎       | 297M/1.31G [00:02<00:07, 140Mbyte/s]\n",
      "Writing:  24%|██▍       | 315M/1.31G [00:02<00:07, 141Mbyte/s]\n",
      "Writing:  25%|██▌       | 333M/1.31G [00:02<00:06, 142Mbyte/s]\n",
      "Writing:  28%|██▊       | 362M/1.31G [00:02<00:06, 144Mbyte/s]\n",
      "Writing:  29%|██▉       | 379M/1.31G [00:02<00:06, 144Mbyte/s]\n",
      "Writing:  30%|███       | 397M/1.31G [00:02<00:06, 144Mbyte/s]\n",
      "Writing:  32%|███▏      | 426M/1.31G [00:02<00:06, 147Mbyte/s]\n",
      "Writing:  34%|███▍      | 444M/1.31G [00:03<00:05, 147Mbyte/s]\n",
      "Writing:  35%|███▌      | 462M/1.31G [00:03<00:05, 147Mbyte/s]\n",
      "Writing:  37%|███▋      | 491M/1.31G [00:03<00:05, 149Mbyte/s]\n",
      "Writing:  39%|███▊      | 509M/1.31G [00:03<00:05, 150Mbyte/s]\n",
      "Writing:  40%|████      | 526M/1.31G [00:03<00:05, 149Mbyte/s]\n",
      "Writing:  42%|████▏     | 555M/1.31G [00:03<00:05, 151Mbyte/s]\n",
      "Writing:  44%|████▎     | 573M/1.31G [00:03<00:04, 150Mbyte/s]\n",
      "Writing:  45%|████▌     | 591M/1.31G [00:04<00:04, 149Mbyte/s]\n",
      "Writing:  47%|████▋     | 620M/1.31G [00:04<00:04, 150Mbyte/s]\n",
      "Writing:  49%|████▊     | 638M/1.31G [00:04<00:04, 149Mbyte/s]\n",
      "Writing:  50%|████▉     | 656M/1.31G [00:04<00:04, 148Mbyte/s]\n",
      "Writing:  52%|█████▏    | 685M/1.31G [00:04<00:04, 150Mbyte/s]\n",
      "Writing:  53%|█████▎    | 703M/1.31G [00:04<00:04, 148Mbyte/s]\n",
      "Writing:  55%|█████▍    | 720M/1.31G [00:04<00:04, 148Mbyte/s]\n",
      "Writing:  57%|█████▋    | 749M/1.31G [00:05<00:03, 150Mbyte/s]\n",
      "Writing:  58%|█████▊    | 767M/1.31G [00:05<00:03, 149Mbyte/s]\n",
      "Writing:  60%|█████▉    | 785M/1.31G [00:05<00:03, 148Mbyte/s]\n",
      "Writing:  62%|██████▏   | 814M/1.31G [00:05<00:03, 150Mbyte/s]\n",
      "Writing:  63%|██████▎   | 832M/1.31G [00:05<00:03, 149Mbyte/s]\n",
      "Writing:  65%|██████▍   | 850M/1.31G [00:05<00:03, 148Mbyte/s]\n",
      "Writing:  67%|██████▋   | 879M/1.31G [00:06<00:02, 149Mbyte/s]\n",
      "Writing:  68%|██████▊   | 896M/1.31G [00:06<00:02, 149Mbyte/s]\n",
      "Writing:  70%|██████▉   | 914M/1.31G [00:06<00:02, 148Mbyte/s]\n",
      "Writing:  72%|███████▏  | 943M/1.31G [00:06<00:02, 149Mbyte/s]\n",
      "Writing:  73%|███████▎  | 961M/1.31G [00:06<00:02, 148Mbyte/s]\n",
      "Writing:  75%|███████▍  | 979M/1.31G [00:06<00:02, 147Mbyte/s]\n",
      "Writing:  77%|███████▋  | 1.01G/1.31G [00:06<00:02, 148Mbyte/s]\n",
      "Writing:  78%|███████▊  | 1.03G/1.31G [00:07<00:01, 148Mbyte/s]\n",
      "Writing:  79%|███████▉  | 1.04G/1.31G [00:07<00:01, 147Mbyte/s]\n",
      "Writing:  82%|████████▏ | 1.07G/1.31G [00:07<00:01, 149Mbyte/s]\n",
      "Writing:  83%|████████▎ | 1.09G/1.31G [00:07<00:01, 147Mbyte/s]\n",
      "Writing:  84%|████████▍ | 1.11G/1.31G [00:07<00:01, 147Mbyte/s]\n",
      "Writing:  87%|████████▋ | 1.14G/1.31G [00:07<00:01, 150Mbyte/s]\n",
      "Writing:  88%|████████▊ | 1.15G/1.31G [00:07<00:01, 149Mbyte/s]\n",
      "Writing:  89%|████████▉ | 1.17G/1.31G [00:08<00:00, 148Mbyte/s]\n",
      "Writing:  92%|█████████▏| 1.20G/1.31G [00:08<00:00, 150Mbyte/s]\n",
      "Writing:  93%|█████████▎| 1.22G/1.31G [00:08<00:00, 149Mbyte/s]\n",
      "Writing:  94%|█████████▍| 1.24G/1.31G [00:08<00:00, 148Mbyte/s]\n",
      "Writing:  96%|█████████▋| 1.27G/1.31G [00:08<00:00, 150Mbyte/s]\n",
      "Writing:  98%|█████████▊| 1.28G/1.31G [00:08<00:00, 150Mbyte/s]\n",
      "Writing:  99%|█████████▉| 1.30G/1.31G [00:08<00:00, 149Mbyte/s]\n",
      "Writing: 100%|██████████| 1.31G/1.31G [00:09<00:00, 143Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to converted\\gguf_model.gguf\n"
     ]
    }
   ],
   "source": [
    "!mkdir converted && python dependencies/llama.cpp/convert_hf_to_gguf.py model --outfile converted/gguf_model.gguf --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = os.path.abspath(\"converted/gguf_model.gguf\")\n",
    "with open(\"converted/Modelfile\", \"w+\") as file:\n",
    "    file.write(f\"from {model_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.create(\"our_model\", \"./converted/Modelfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, multiply the numbers 3 and 3. The product of 3 x 3 is 9. Then subtract 15 from 9. The result of this subtraction operation is 0. Therefore, 3 x 3 - 15 equals 0.\n"
     ]
    }
   ],
   "source": [
    "answer = ollama.generate(\"our_model\", \"What is 3 x 3 - 15?\")\n",
    "print(answer[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Formats explained\n",
    "\n",
    "[Detailed Formats](https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/#finally-gguf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative Conversion with Triton\n",
    "\n",
    "[Native Conversion Unsloph](https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing#scrollTo=IqM-T1RTzY6C)\n",
    "\n",
    "[Unsloph](https://github.com/unslothai/unsloth)\n",
    "\n",
    "[Requirement](https://github.com/woct0rdho/triton-windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
