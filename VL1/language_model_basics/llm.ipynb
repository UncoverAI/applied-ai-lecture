{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right; margin:0 300px 0 0\" src=\"./DecoderLLM.png\">\n",
    "\n",
    "## Large Language Models (LLMs)\n",
    "\n",
    "The field of Natural Language Processing (NLP) has a lot more to offer than just \"Language Models\" that generate text, but to keep it simple we will focus on that part.\n",
    "\n",
    "To understand how Large Language Models (LLMs) work a foundation in Deep Learning is necessary as well, a great YouTube series that goes from basic Deep Learning up until LLMs, can be found here: [3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "\n",
    "### What we need for this lecture\n",
    "\n",
    "1. Python, IDE & Notebook\n",
    "2. Huggingface for model access (https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct)\n",
    "\n",
    "### A full Language Model example\n",
    "\n",
    "We will use a \"Small Language Model\" as we can run it easily on our CPU to do some fun first experiments. \n",
    "\n",
    "Not sure if there even as a threshold when to consider a model \"large\", but for now let's call models above like 7 billion parameter \"Large Language Model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lets start by installing and importing the packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages, classes & functions\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import no_grad, argmax\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Next we load the model - you might need to restart your notebook kernel after installation for this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy model_id from huggingface\n",
    "huggingface_model_id = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "# set device to cpu, for gpu usage we could use \"cuda\"\n",
    "device = \"cpu\"\n",
    "# load the tokenizer, we will learn in a second what that is\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model_id)\n",
    "# load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(huggingface_model_id).to(device)\n",
    "# print the model, to take a first look at it\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define messages\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "# Add special tokens\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "# Tokenize input\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "# Call the model to generate new tokens\n",
    "outputs = model.generate(inputs)\n",
    "# \"Detokenize\" output\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking it down step by step - each layer of a language model explained\n",
    "\n",
    "![Model Architecture](./DecoderLLM_N.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization\n",
    "\n",
    "is the process of splitting a piece of text into smaller units, called tokens. These tokens can be words, characters, or subwords, depending on the level of granularity you want. In the context of NLP, tokens are typically words or meaningful units of text. For Language Models words or subwords are often used. For a model to be able to use these tokens they need to be mapped to a number.\n",
    "\n",
    "The simplest tokenizer would be a lookup table for characters. In a \"useful\" LM tokenizers are a bit more complex and usually use subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(tokenizer.vocab.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Tokens to Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab[\"Hello\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: tokenizer.vocab[\"Hell\"]\n",
    "except Exception as e: print(type(e),str(e))\n",
    "\n",
    "print(tokenizer.encode(\"Hell\"))\n",
    "print(tokenizer.vocab[\"H\"])\n",
    "print(tokenizer.vocab[\"ell\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"Hello\"))\n",
    "print(tokenizer.encode(\"Hello,\"))\n",
    "print(tokenizer.encode(\"Hello, World\"))\n",
    "print(tokenizer.encode(\"Hello, World!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Tokens for Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.vocab[\"<|im_start|>\"])\n",
    "print(tokenizer.vocab[\"<|im_end|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of France.\"}]\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of France.\"}, {\"role\": \"assistant\", \"content\": \"It is Paris.\"}]\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))\n",
    "print(tokenizer.apply_chat_template(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Embeddings\n",
    "In very simple terms, an embedding is a `multi-dimensional vector` that is supposed to ``encode the meaning of the text`` it belongs to.  \n",
    "The embedding layer of the model we investigate here has an ``embedding dimension of 576`` - so a vector of length 576 for each token.  \n",
    "In theory you can try and think of each dimension in an embedding vector to represent an attribute of the word it is encoding, e.g. is it male?, is it a noun?, is it a country?, ...  \n",
    "In practice it is unclear what exactly this embeddings represent and there is a large body of research around making these things more explainable.\n",
    "\n",
    "In this section we take a look at the embedding layer of our model and try to visualize what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tokenizer to get IDs\n",
    "input_ids = tokenizer.encode(\"Hello, World!\", return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding are Vectors - 1 Vector for each Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embedding layer from the model\n",
    "embedding_layer = model.model.embed_tokens\n",
    "# Get the output after the embedding layer\n",
    "with no_grad():\n",
    "    embeddings_output = embedding_layer(input_ids)\n",
    "print(embeddings_output.shape)\n",
    "embeddings_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's play a bit with these vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Cat\",\"Dog\",\"Bird\",\"Machine\",\"Car\"]\n",
    "\n",
    "sentences = [\"The cat sat on the table.\", \"The hamster lay on the mat.\", \"Birds fly high in the sky.\",\n",
    "             \"Cats fly high in the sky.\", \"Machine Learning is cool.\"]\n",
    "\n",
    "def create_embeddings(inputs: list):\n",
    "    # Tokenize and encode the sentences\n",
    "    input_ids = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(device)\n",
    "    with no_grad():\n",
    "        embeddings = model.model.embed_tokens(input_ids)\n",
    "    print(embeddings.size())\n",
    "    # Average embeddings across the sequence length in case of sentences\n",
    "    embeddings = embeddings.mean(dim=1).cpu().numpy()\n",
    "    # embeddings = embeddings.reshape(-1, embeddings.size()[-1])\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n",
    "def plot_embeddings(inputs, embeddings, compare_embeddings = None):\n",
    "    # Reduce dimensions to 2D using PCA since we can only plot 2D vectors\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    # Plot the embeddings\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=100)\n",
    "\n",
    "    # Annotate points with the sentences\n",
    "    for i, elem in enumerate(inputs):\n",
    "        plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], elem, fontsize=12)\n",
    "    \n",
    "    if compare_embeddings is not None:\n",
    "        reduced_compare_embeddings = pca.fit_transform(compare_embeddings)\n",
    "        plt.scatter(reduced_compare_embeddings[:, 0], reduced_compare_embeddings[:, 1], c=\"green\", s=100)\n",
    "\n",
    "\n",
    "    plt.title(\"2-D PCA of Embeddings\")\n",
    "    plt.xticks([-2, -1, 0, 1, 2]) # Set fixed x-axis ticks\n",
    "    # plt.yticks([-2, -1, 0, 1, 2]) # Set fixed y-axis ticks\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_embeddings(words, create_embeddings(words))\n",
    "\n",
    "plot_embeddings(sentences, create_embeddings(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_sentences = [' '.join(random.sample(sentence.split(), len(sentence.split()))) for sentence in sentences]\n",
    "plot_embeddings(shuffled_sentences, create_embeddings(shuffled_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Positional Encoding\n",
    "\n",
    "As you can see in the last example, the output of our embedding layer has a problem: It does not encode the order of words.\n",
    "But to properly encode the full information of a sentence the order of words matters a lot. And since our LLM is supposed to correctly continue a sentence it needs to perfectly understand its meaning.\n",
    "\n",
    "To resolve the missing information of order, LLMs use a Position Encoding that is \"added\" to the Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic Positional Encoding with Adjustable Constant \\( C \\)\n",
    "\n",
    "To control the influence of positional information in transformer models, we use the following modified positional encoding formulas:\n",
    "\n",
    "- Positional Encoding for Even Dimensions (\\( i = 2k \\)):\n",
    "\n",
    "$\n",
    "PE_{(pos, 2k)} = \\sin\\left(\\frac{pos}{C^{\\frac{2k}{d\\_model}}}\\right)\n",
    "$\n",
    "\n",
    "- Positional Encoding for Odd Dimensions (\\( i = 2k+1 \\)):\n",
    "\n",
    "$\n",
    "PE_{(pos, 2k+1)} = \\cos\\left(\\frac{pos}{C^{\\frac{2k}{d\\_model}}}\\right)\n",
    "$\n",
    "\n",
    "- The constant \\( C \\) controls the scaling of positional encoding frequencies across dimensions.\n",
    "- **Larger \\( C \\) Values**: Result in slower changes in frequency across dimensions, emphasizing broader contextual information and capturing long-range dependencies.\n",
    "- **Smaller \\( C \\) Values**: Result in faster changes in frequency, providing more detailed positional information and helping distinguish close-by positions more effectively.  \n",
    "The original formula from the Transformer paper uses C=10000 we use C=1 to better visualize our short text examples.\n",
    "\n",
    "#### RoPE - improved Positional Encoding\n",
    "Since the model we investigate here is a state of the art LLM it uses [RoPE](https://arxiv.org/abs/2104.09864). But even though it works better, it is harder to visualize, so to explain the concept we use the classical one for the next few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_positional_encoding(embeddings):\n",
    "    # This is just the code for the above defined formulas\n",
    "    def positional_encoding(sequence_length, embedding_dimension):\n",
    "        pos = np.arange(sequence_length)[:, np.newaxis]\n",
    "        i = np.arange(embedding_dimension)[np.newaxis, :]\n",
    "        oscillator = 1\n",
    "        angles = pos / np.power(oscillator, (2 * (i // 2)) / np.float32(embedding_dimension))\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        return angles\n",
    "    # We then multiply that positional encoding with the original embedding vector\n",
    "    return embeddings * positional_encoding(embeddings.shape[0], embeddings.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(sentences, create_embeddings(sentences), create_embeddings(shuffled_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(sentences, add_positional_encoding(create_embeddings(sentences)), add_positional_encoding(create_embeddings(shuffled_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Attention\n",
    "\n",
    "``Attention``is a mechanism used in machine learning, especially in NLP, to help models focus on the most relevant parts of the input data when making decisions.  \n",
    "Imagine you are reading a long text; instead of trying to understand everything at once, attention allows you to selectively focus on key words or sentences that are crucial for understanding the context.  \n",
    "Similarly, in neural networks, attention helps the model to give different levels of importance to different parts of the input.  \n",
    "This improves its ability to handle complex tasks like translation or summarization by effectively \"paying attention\" to the right information at the right time.\n",
    "\n",
    "It was orignially invented in the new very famous (over 135.000 citations) paper [Attention is all you need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some code to help visualize what attention does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_output(output):\n",
    "    # Assume you have the following logits from your model output with Shape: [1, sequence_length, vocab_size]\n",
    "    # Step 1: Extract the logits for the last token in the sequence\n",
    "    last_token_logits = output.logits[:, -1, :]  # Shape: [1, vocab_size]\n",
    "    # Step 2: Apply softmax to get probabilities\n",
    "    probs = F.softmax(last_token_logits, dim=-1)  # Shape: [1, vocab_size]\n",
    "    # Step 3: Get the token with the highest probability\n",
    "    predicted_token_id = argmax(probs, dim=-1)  # Shape: [1]\n",
    "    # Convert the token id to a human-readable token (if needed)\n",
    "    predicted_token = tokenizer.decode(predicted_token_id[0].item())\n",
    "    return predicted_token\n",
    "\n",
    "# Hook to store weights of attention during forward pass\n",
    "class AttentionWeightsHook:\n",
    "    def __init__(self):\n",
    "        self.attention_layer_outputs = []\n",
    "    \n",
    "    def __call__(self, module, input, output):\n",
    "        self.attention_layer_outputs.append(output[0])\n",
    "\n",
    "def forward_with_attention(input_ids):   \n",
    "    # Create hook and attach it to the attention layers\n",
    "    hook = AttentionWeightsHook()\n",
    "    hooks = []\n",
    "    for layer in model.model.layers:\n",
    "        hook_handle = layer.self_attn.register_forward_hook(hook)\n",
    "        hooks.append(hook_handle)\n",
    "    # Forward pass\n",
    "    with no_grad():\n",
    "        predicted_token = decode_output(model(input_ids))\n",
    "    # Remove hooks after the forward pass\n",
    "    for handle in hooks:\n",
    "        handle.remove()\n",
    "\n",
    "    return predicted_token, hook.attention_layer_outputs\n",
    "\n",
    "def plot_attention_layer(attention, input_ids, layer_idx):\n",
    "    # Extract the weights for the specific layer and head   \n",
    "    layer_ouput = attention[layer_idx].squeeze()\n",
    "    # multiply the output of the attention layer with it's transpose to get the right output size\n",
    "    att = layer_ouput @ layer_ouput.T \n",
    "    # att = att * (1 - triu(ones(att.shape[0], att.shape[1]), diagonal=1)) # mask out attention so tokens can not look into the future\n",
    "    tokens = [tokenizer.decode(_id) for _id in input_ids.squeeze()]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(att.numpy(), xticklabels=tokens, yticklabels=tokens)\n",
    "    plt.title(f'Attention Layer Output - Layer {layer_idx}')\n",
    "    plt.xlabel('Input Token')\n",
    "    plt.ylabel('Input Token')\n",
    "    plt.show()\n",
    "\n",
    "def plot_attention_weights(attention, input_ids, layer_idx):\n",
    "    tokens = [tokenizer.decode(_id) for _id in input_ids.squeeze()]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention.numpy(), xticklabels=tokens, yticklabels=tokens)\n",
    "    plt.title(f'Attention Weights - Layer {layer_idx}')\n",
    "    plt.xlabel('Input Token')\n",
    "    plt.ylabel('Input Token')\n",
    "    plt.show()\n",
    "\n",
    "def generate_w_attention_weights_plot(input_sentence, layer_idx=-1, num_tokens = 1):\n",
    "    inputs = tokenizer.encode(input_sentence, return_tensors=\"pt\").to(device)\n",
    "    # output_attentions=True, return_dict_in_generate=True let you see the attention weights as part of the output\n",
    "    outputs = model.generate(inputs, max_new_tokens=num_tokens, output_attentions=True, return_dict_in_generate=True)\n",
    "    attention = outputs[\"attentions\"][0][layer_idx].squeeze().mean(dim=0)\n",
    "    plot_attention_weights(attention, inputs, layer_idx)\n",
    "\n",
    "def generate_w_attention_layer_plot(input_sentence, layer_idx, num_tokens = 1, animate = False):\n",
    "    for i in range(num_tokens):\n",
    "        input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
    "        predicted_token, attention_layer_output = forward_with_attention(input_ids)\n",
    "        plot_attention_layer(attention_layer_output, input_ids, layer_idx)\n",
    "        print(f\"Predicted token: {predicted_token}\")\n",
    "        input_sentence += predicted_token\n",
    "        if animate:\n",
    "            sleep(0.2)\n",
    "            clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A concrete example to show attention works\n",
    "\n",
    "You can change the layer_idx to -1 to see the output of the last attention layer which also explains why the first 2 examples still work (output is 4) and the last one does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "layer_idx = 0  # Index of the attention layer\n",
    "input_sentence = \" 2 + 2 = \"\n",
    "generate_w_attention_weights_plot(input_sentence, layer_idx, num_tokens=1)\n",
    "generate_w_attention_layer_plot(input_sentence, layer_idx, num_tokens=1)\n",
    "input_sentence = \" noise noise noise 2 + 2 = \"\n",
    "generate_w_attention_weights_plot(input_sentence, layer_idx, num_tokens=1)\n",
    "generate_w_attention_layer_plot(input_sentence, layer_idx, num_tokens=1)\n",
    "input_sentence = \" 2 + 2 noise noise noise = \"\n",
    "generate_w_attention_weights_plot(input_sentence, layer_idx, num_tokens=1)\n",
    "generate_w_attention_layer_plot(input_sentence, layer_idx, num_tokens=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multi Layer Perceptron\n",
    "\n",
    "Looking model architecture, within each `LlamaDecoderLayer`, after **Attention** there is an **MLP block** which plays a critical role in processing information after the self-attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's a breakdown of the MLP Blocks function:\n",
    "\n",
    "After the self-attention mechanism identifies and processes the relevant parts of the input, the **MLP (Multi-Layer Perceptron) block** helps to further transform and refine the information. It does this by applying a series of linear transformations and non-linear activations to the output from the attention layer. In the Llama model, the MLP block consists of:\n",
    "\n",
    "1. **Linear Layers (Gate, Up, and Down Projections)**: \n",
    "   - The `gate_proj` and `up_proj` layers are used to increase the dimensionality of the input (from 576 to 1536 in this case), allowing the network to learn more complex representations. \n",
    "   - The `down_proj` layer then reduces the dimensionality back to the original size (from 1536 to 576), ensuring the output size matches the input size of the layer, which is necessary for the residual connections (skip connections) to work properly.\n",
    "\n",
    "2. **Activation Function (SiLU)**: \n",
    "   - The activation function introduces non-linearity, enabling the network to learn more complex patterns. The SiLU (Sigmoid Linear Unit) function is used here, which is a smooth and differentiable activation function that performs well in deep networks.\n",
    "\n",
    "##### Why is it Important\n",
    "\n",
    "The MLP block allows the model to combine and re-integrate the information captured by the attention mechanism in a more expressive and powerful way. While the attention mechanism excels at finding relationships and dependencies in the data, the MLP block refines these insights by applying learned transformations, enabling the model to make nuanced and accurate predictions. Together, the self-attention and MLP blocks enable the model to effectively process and generate text, making them essential components of modern transformer-based architectures like Llama.\n",
    "\n",
    "This is difficult to visualize we will attempt to do so by looking at the first (0) and last layer (29) of the Decoder Blocks to see how the attion changes when being processed by all the MLP Blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \" 2 + 2 - 3 = \"\n",
    "generate_w_attention_layer_plot(input_sentence, layer_idx=0, num_tokens=15, animate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_w_attention_layer_plot(input_sentence, layer_idx=29, num_tokens=15, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "In the first layer of attention the model has grasped roughly how to inputs belong together, but looking at the last layer you can clearly see that for example the whitespace between the first equeation and the second in starts to generate does not need to be attended to as it contains no information.  \n",
    "This is the kind of information that is stored/learned within the weights of the MLP blocks, \"whitespaces do not matter for mathematical equations\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. The Linear layer\n",
    "\n",
    "The `Linear` layer in a language model is the final step in the model that converts the abstract representations generated by the neural network into actual words or tokens. Here's how it works:\n",
    "\n",
    "1. **Purpose**: After the model processes the input text through various layers, including attention and MLP blocks, it generates a numerical representation of what it believes should come next in the sequence. The `Linear` layer transforms these numerical representations into probabilities for each possible word or token in the vocabulary.\n",
    "\n",
    "2. **Linear Transformation**: The `Linear` layer is a linear layer (a type of fully connected layer), which means it performs a matrix multiplication to convert the final hidden state (of size 576 in this case) into logits --> a raw score for each token in the vocabulary. This is done using a weight matrix of size 576 by 49152, where 49152 corresponds to the size of the vocabulary.\n",
    "\n",
    "3. **Output**: The output of the `Linear` layer is a vector of scores, each representing the likelihood of a particular token being the next one in the sequence. These scores are then typically passed through a softmax function to turn them into probabilities, which are used to predict the next token in tasks like text generation.\n",
    "\n",
    "In summary, the `Linear` layer is crucial because it takes the complex internal representations created by the model and maps them to a space where they can be interpreted as meaningful predictions about which words or tokens are likely to come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_next_token_probability(input_sentence, top_k = 20):\n",
    "    input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
    "    with no_grad(): \n",
    "        output = model(input_ids)\n",
    "    # Extract the logits for the last token\n",
    "    last_token_logits = output.logits[:, -1, :]  # Shape: [1, vocab_size]\n",
    "    # Convert logits to probabilities\n",
    "    probs = F.softmax(last_token_logits, dim=-1)  # Shape: [1, vocab_size]\n",
    "    # Convert probabilities to numpy array for visualization\n",
    "    probs = probs.squeeze().numpy()\n",
    "    top_k_indices = np.argsort(probs)[-top_k:]\n",
    "    top_k_probs = probs[top_k_indices]\n",
    "    top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(top_k_tokens, top_k_probs, color='skyblue', edgecolor='black')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel('Token')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'Top {top_k} Token Probabilities')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \" 2 + 2 = \"\n",
    "plot_next_token_probability(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \" 2 + 2 = \"\n",
    "messages = [{\"role\": \"user\", \"content\": input_sentence}]\n",
    "input_sentence=tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(input_sentence)\n",
    "plot_next_token_probability(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"Who are you?\"\n",
    "messages = [{\"role\": \"user\", \"content\": input_sentence}]\n",
    "input_sentence=tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(input_sentence)\n",
    "plot_next_token_probability(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \" 2 + 2 = \"\n",
    "messages = [{\"role\": \"user\", \"content\": input_sentence}]\n",
    "input_sentence=tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(input_sentence)\n",
    "input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(input_ids, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
