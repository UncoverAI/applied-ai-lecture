{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'nomic-embed-text:latest',\n",
       "   'model': 'nomic-embed-text:latest',\n",
       "   'modified_at': '2024-10-19T12:36:50.1537198+02:00',\n",
       "   'size': 274302450,\n",
       "   'digest': '0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'nomic-bert',\n",
       "    'families': ['nomic-bert'],\n",
       "    'parameter_size': '137M',\n",
       "    'quantization_level': 'F16'}},\n",
       "  {'name': 'llama3.1:8b-instruct-q4_0',\n",
       "   'model': 'llama3.1:8b-instruct-q4_0',\n",
       "   'modified_at': '2024-10-19T12:36:49.5810278+02:00',\n",
       "   'size': 4661230766,\n",
       "   'digest': '42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'codestral:latest',\n",
       "   'model': 'codestral:latest',\n",
       "   'modified_at': '2024-09-20T19:34:45.1339781+02:00',\n",
       "   'size': 12569170438,\n",
       "   'digest': '0898a8b286d56d8105587049fec69634fce83c957230fc13f0acfe03b7b11909',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '22.2B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'llama3.1:8b-instruct-fp16',\n",
       "   'model': 'llama3.1:8b-instruct-fp16',\n",
       "   'modified_at': '2024-08-04T13:39:44.1904459+02:00',\n",
       "   'size': 16068905889,\n",
       "   'digest': '9d95e89188d4315cedc62d969a7b4257cce295797013044c4094823c3ced502f',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'F16'}}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'llama3.1:8b-instruct-q4_0',\n",
       "   'model': 'llama3.1:8b-instruct-q4_0',\n",
       "   'size': 6654289920,\n",
       "   'digest': '42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'},\n",
       "   'expires_at': '2024-10-20T13:05:39.3595944+02:00',\n",
       "   'size_vram': 6654289920}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.ps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LM_MODEL_ID = 'llama3.1:8b-instruct-q4_0'\n",
    "EMBED_MODEL_ID=\"nomic-embed-text\"\n",
    "ollama.pull(LM_MODEL_ID)\n",
    "ollama.pull(EMBED_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "text = \"Cars are cool!\"\n",
    "response = ollama.embed(model=EMBED_MODEL_ID, input=text, keep_alive=\"24h\")\n",
    "print(len(response[\"embeddings\"]))\n",
    "print(len(response[\"embeddings\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "768\n",
      "768\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Cars are cool!\", \"Trucks are even cooler!\", \"Cats have cool eyes! And this text is longer then the previous two.\"]\n",
    "response = ollama.embed(model=EMBED_MODEL_ID, input=texts)\n",
    "print(len(response[\"embeddings\"]))\n",
    "for embedding in response[\"embeddings\"]:\n",
    "    print(len(embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.1:8b-instruct-q4_0', 'created_at': '2024-10-19T10:36:29.3195336Z', 'response': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"', 'done': True, 'done_reason': 'stop', 'context': [128006, 882, 128007, 271, 9906, 11, 889, 527, 499, 30, 128009, 128006, 78191, 128007, 271, 40, 2846, 459, 21075, 11478, 1646, 3967, 439, 445, 81101, 13, 445, 81101, 13656, 369, 330, 35353, 11688, 5008, 16197, 15592, 1210], 'total_duration': 337081100, 'load_duration': 19001100, 'prompt_eval_count': 16, 'prompt_eval_duration': 22814000, 'eval_count': 23, 'eval_duration': 293276000}\n",
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(model=LM_MODEL_ID, prompt=\"Hello, who are you?\")\n",
    "print(response)\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.1:8b-instruct-q4_0', 'created_at': '2024-10-19T10:36:31.2514415Z', 'message': {'role': 'assistant', 'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"'}, 'done_reason': 'stop', 'done': True, 'total_duration': 304999700, 'load_duration': 19504300, 'prompt_eval_count': 16, 'prompt_eval_duration': 16232000, 'eval_count': 23, 'eval_duration': 268629000}\n",
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model=LM_MODEL_ID, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Hello, who are you?',\n",
    "  },\n",
    "])\n",
    "print(response)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.1:8b-instruct-q4_0', 'created_at': '2024-10-19T10:46:54.6792577Z', 'message': {'role': 'assistant', 'content': \"Ich bin ein Computer-Programm, das darauf programmiert wurde, auf Deutsch zu antworten. Ich habe keine persönlichen Erfahrungen oder Emotionen und bin hier, um Fragen zu beantworten und Informationen bereitzustellen! (I'm a computer program that's been programmed to respond in German. I don't have any personal experiences or emotions and am here to answer questions and provide information!)\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 1416812400, 'load_duration': 20634300, 'prompt_eval_count': 35, 'prompt_eval_duration': 214553000, 'eval_count': 85, 'eval_duration': 1179307000}\n",
      "Ich bin ein Computer-Programm, das darauf programmiert wurde, auf Deutsch zu antworten. Ich habe keine persönlichen Erfahrungen oder Emotionen und bin hier, um Fragen zu beantworten und Informationen bereitzustellen! (I'm a computer program that's been programmed to respond in German. I don't have any personal experiences or emotions and am here to answer questions and provide information!)\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model=LM_MODEL_ID, messages=[\n",
    "  {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"ALWAYS ANSWER IN GERMAN, NO MATTER THE QUESTION!\"\n",
    "  },\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Hello, who are you?',\n",
    "  },\n",
    "])\n",
    "print(response)\n",
    "print(response['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncoverai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
